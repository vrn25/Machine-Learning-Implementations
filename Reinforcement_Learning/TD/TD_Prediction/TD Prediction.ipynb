{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "####################### GRIDWORLD ###########################\n",
    "    GridWorld like:\n",
    "\n",
    "    T o o ..........o\n",
    "    o o o ..........o\n",
    "    . .             .        \n",
    "    . .             .  \n",
    "    . .             o\n",
    "    . .             o\n",
    "    o o o ......o o T\n",
    "Actions: \n",
    "    UP (0)\n",
    "    DOWN (1)\n",
    "    RIGHT (2)\n",
    "    LEFT (3)\n",
    "\n",
    "Rewards: \n",
    "     0 for going in Terminal state\n",
    "    -1 for all other actions in any state\n",
    "\n",
    "Note: State remains the same on going out of the maze (but -1 reward is given)\n",
    "'''\n",
    "def env(state, action):\n",
    "    # return_val = [prob, next state, reward, isdone]\n",
    "    num_states = rows * columns\n",
    "    isdone = lambda state: state==0 or state==(num_states-1)\n",
    "    reward = lambda state: 0 if isdone(state) else -1\n",
    "\n",
    "    if(isdone(state)):\n",
    "        next_state = state\n",
    "    else:\n",
    "        if(action==0):\n",
    "            next_state = state-columns if state-columns>=0 else state\n",
    "        elif(action==1):\n",
    "            next_state = state+columns if state+columns<num_states else state\n",
    "        elif(action==2):\n",
    "            next_state = state+1 if (state+1)%columns else state\n",
    "        elif(action==3):\n",
    "            next_state = state-1 if state%columns else state \n",
    "    # State Transition Probability is 1 because the environment is deterministic\n",
    "    return_val = [1, next_state, reward(next_state), isdone(next_state)]\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_pred(policy, flag):\n",
    "    # Initialize Value function\n",
    "    VF = np.zeros(num_states)\n",
    "    for episode in range(episodes):\n",
    "        # Initialize S\n",
    "        curr_state = np.random.randint(0, num_states)\n",
    "        while True:\n",
    "            # Sample an action from S\n",
    "            curr_action = np.argmax(policy[curr_state]) if flag else np.random.randint(0, num_actions)\n",
    "            # prob: State Transition Probability \n",
    "            # reward, next_state: Immediate reward and next state on taking curr_action in curr_state\n",
    "            # isdone: Whether the next state is Terminal or not\n",
    "            prob, next_state, reward, isdone = env(curr_state, curr_action)\n",
    "            # Update the current state value\n",
    "            VF[curr_state] = VF[curr_state] + alpha * (reward + gamma * VF[next_state] - VF[curr_state])\n",
    "            curr_state = next_state\n",
    "            if isdone:\n",
    "                break\n",
    "    return VF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Learning Rate\n",
    "rows = 5\n",
    "columns = 7\n",
    "num_states = rows * columns\n",
    "num_actions = 4\n",
    "gamma = 0.99 # Discount Factor\n",
    "episodes = 100000 # Number of games played\n",
    "# UNIFORM RANDOM POLICY\n",
    "rand_policy = np.ones((num_states, num_actions))/num_actions\n",
    "# GREEDY DETERMINISTIC POLICY \n",
    "deter_policy = [[1, 0, 0, 0],[0, 0, 0, 1],[0, 0, 0, 1],[0, 0, 0, 1],[0, 0, 0, 1],[0, 1, 0, 0],[0, 1, 0, 0],\n",
    "          [1, 0, 0, 0],[1, 0, 0, 0],[1, 0, 0, 0],[1, 0, 0, 0],[1, 0, 0, 0],[0, 1, 0, 0],[0, 1, 0, 0],\n",
    "          [1, 0, 0, 0],[1, 0, 0, 0],[1, 0, 0, 0],[1, 0, 0, 0],[0, 1, 0, 0],[0, 1, 0, 0],[0, 1, 0, 0],\n",
    "          [1, 0, 0, 0],[1, 0, 0, 0],[1, 0, 0, 0],[0, 1, 0, 0],[0, 1, 0, 0],[0, 1, 0, 0],[0, 1, 0, 0],\n",
    "          [1, 0, 0, 0],[1, 0, 0, 0],[0, 0, 1, 0],[0, 0, 1, 0],[0, 0, 1, 0],[0, 0, 1, 0],[1, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function for Uniform Random policy:\n",
      " [[  0.         -26.70110685 -33.72970448 -37.31734068 -38.96098171\n",
      "  -39.49996922 -39.46662963]\n",
      " [-18.63437237 -29.20014148 -35.36728293 -37.96337221 -38.22157014\n",
      "  -37.24253909 -36.6876679 ]\n",
      " [-27.70288602 -34.23803591 -36.69153357 -37.53295002 -37.06745825\n",
      "  -33.57396437 -29.4488588 ]\n",
      " [-35.63237252 -37.25021585 -38.13534024 -36.12940037 -33.878469\n",
      "  -23.91097296 -21.03887742]\n",
      " [-38.24304362 -38.56195133 -38.35172435 -35.86120631 -27.5404141\n",
      "  -20.5893415    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Flag is 0 for random policy and 1 for deterministic policy\n",
    "VF = td_pred(rand_policy, flag = 0)\n",
    "print(f'Value Function for Uniform Random policy:\\n {VF.reshape(rows, columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function for deterministic greedy policy:\n",
      " [[ 0.        0.       -1.       -1.99     -2.9701   -3.940399 -2.9701  ]\n",
      " [ 0.       -1.       -1.99     -2.9701   -3.940399 -2.9701   -1.99    ]\n",
      " [-1.       -1.99     -2.9701   -3.940399 -2.9701   -1.99     -1.      ]\n",
      " [-1.99     -2.9701   -3.940399 -2.9701   -1.99     -1.        0.      ]\n",
      " [-2.9701   -3.940399 -2.9701   -1.99     -1.        0.        0.      ]]\n"
     ]
    }
   ],
   "source": [
    "VF = td_pred(deter_policy, flag = 1)\n",
    "print(f'Value Function for deterministic greedy policy:\\n {VF.reshape(rows, columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
