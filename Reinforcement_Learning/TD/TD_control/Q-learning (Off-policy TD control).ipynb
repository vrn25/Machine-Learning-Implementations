{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "####################CLIFF WALKING ENVIRONMENT#########################\n",
    "\n",
    "A schematic view of the environment-\n",
    "\n",
    "o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o\n",
    "S  x  x  x  x  x  T\n",
    "\n",
    "Actions: \n",
    "    UP (0)\n",
    "    DOWN (1)\n",
    "    RIGHT (2)\n",
    "    LEFT (3)\n",
    "\n",
    "Rewards: \n",
    "     0 for going in Terminal state\n",
    "    -100 for falling in the cliff\n",
    "    -1 for all other actions in any state\n",
    "\n",
    "Note: State remains the same on going out of the maze (but -1 reward is given)\n",
    "      The episode ends and the agent returns to the start state after falling in the cliff\n",
    "\n",
    "'''\n",
    "START_STATE = 36\n",
    "TERMINAL_STATE = 47\n",
    "def reward(state):\n",
    "    if(state == TERMINAL_STATE):\n",
    "        reward = 0\n",
    "    elif(state > START_STATE and state < TERMINAL_STATE):\n",
    "        reward = -100\n",
    "    else:\n",
    "        reward = -1\n",
    "    return reward\n",
    "\n",
    "def env(state, action):\n",
    "    # return_val = [prob, next state, reward, isdone]\n",
    "    num_states = rows * columns\n",
    "    isdone = lambda state: state > START_STATE and state <= TERMINAL_STATE\n",
    "    \n",
    "    if(isdone(state)):\n",
    "        next_state = state\n",
    "    else:\n",
    "        if(action==0):\n",
    "            next_state = state-columns if state-columns>=0 else state\n",
    "        elif(action==1):\n",
    "            next_state = state+columns if state+columns<num_states else state\n",
    "        elif(action==2):\n",
    "            next_state = state+1 if (state+1)%columns else state\n",
    "        elif(action==3):\n",
    "            next_state = state-1 if state%columns else state \n",
    "    # State Transition Probability is 1 because the environment is deterministic\n",
    "    return_val = [1, next_state, reward(next_state), isdone(next_state)]\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Learning Rate\n",
    "epsilon = 0.1 # For Epsilon-greedy policy to balance exploration and exploitation\n",
    "rows = 4\n",
    "columns = 12\n",
    "num_states = rows * columns\n",
    "num_actions = 4\n",
    "gamma = 1 # Discount Factor\n",
    "episodes = 100000 # Number of games played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning():\n",
    "    # Initialize the action value function\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    for episode in range(episodes):\n",
    "        # Initialize S\n",
    "        curr_state = START_STATE\n",
    "        while True:\n",
    "            # Generate a random number between 0 and 1\n",
    "            P = np.random.random()\n",
    "            if(P > epsilon):\n",
    "                # Pick the greedy action\n",
    "                curr_action = np.argmax(Q[curr_state])\n",
    "            else:\n",
    "                # Pick a random action to explore\n",
    "                curr_action = np.random.randint(0, num_actions)\n",
    "            # prob: State Transition Probability \n",
    "            # reward, next_state: Immediate reward and next state on taking curr_action in curr_state\n",
    "            # isdone: Whether the next state is Terminal or not    \n",
    "            prob, next_state, reward, isdone = env(curr_state, curr_action)\n",
    "            # Update the current state-action value\n",
    "            Q[curr_state, curr_action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[curr_state, curr_action])\n",
    "            curr_state = next_state\n",
    "            if isdone:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      " [[ -12.94362168  -12.92297404  -12.91765322  -12.98086948]\n",
      " [ -12.20518607  -11.97353105  -11.97340154  -12.39101783]\n",
      " [ -11.15738487  -10.9896074   -10.99008776  -11.10067077]\n",
      " [ -10.23340067   -9.99517922   -9.99562576  -10.88078715]\n",
      " [  -9.22534248   -8.99806412   -8.99791942   -9.8341118 ]\n",
      " [  -8.30784849   -7.99930593   -7.99928418   -8.59131066]\n",
      " [  -7.49242829   -6.99967207   -6.9996564    -7.74176099]\n",
      " [  -6.49474385   -5.99983169   -5.99983218   -6.67940256]\n",
      " [  -5.43802304   -4.99992311   -4.99992005   -6.04972726]\n",
      " [  -4.33752367   -3.99997412   -3.99997054   -4.72241688]\n",
      " [  -3.3308109    -2.99999181   -2.99999096   -3.79557165]\n",
      " [  -2.46100471   -2.           -2.17834858   -2.79399297]\n",
      " [ -13.85899652  -12.          -12.          -12.99939911]\n",
      " [ -12.92766845  -11.          -11.          -12.99983799]\n",
      " [ -11.95720726  -10.          -10.          -11.9977193 ]\n",
      " [ -10.9855592    -9.           -9.          -10.99748233]\n",
      " [  -9.9858338    -8.           -8.           -9.9997052 ]\n",
      " [  -8.99596939   -7.           -7.           -8.9987633 ]\n",
      " [  -7.99705157   -6.           -6.           -7.99924532]\n",
      " [  -6.99835899   -5.           -5.           -6.99899047]\n",
      " [  -5.99574178   -4.           -4.           -5.99940254]\n",
      " [  -4.99839244   -3.           -3.           -4.99952703]\n",
      " [  -3.98987765   -2.           -2.           -3.98688477]\n",
      " [  -2.99938576   -1.           -1.99724591   -2.99918938]\n",
      " [ -13.          -13.          -11.          -12.        ]\n",
      " [ -12.         -100.          -10.          -12.        ]\n",
      " [ -11.         -100.           -9.          -11.        ]\n",
      " [ -10.         -100.           -8.          -10.        ]\n",
      " [  -9.         -100.           -7.           -9.        ]\n",
      " [  -8.         -100.           -6.           -8.        ]\n",
      " [  -7.         -100.           -5.           -7.        ]\n",
      " [  -6.         -100.           -4.           -6.        ]\n",
      " [  -5.         -100.           -3.           -5.        ]\n",
      " [  -4.         -100.           -2.           -4.        ]\n",
      " [  -3.         -100.           -1.           -3.        ]\n",
      " [  -2.            0.           -1.           -2.        ]\n",
      " [ -12.          -13.         -100.          -13.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "\n",
      "\n",
      "Deterministic Policy:\n",
      " [[2 2 1 1 2 2 2 1 2 2 2 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Q = qlearning()\n",
    "# Deterministic policy obtained using updated Q values\n",
    "policy = np.argmax(Q,axis=1)\n",
    "print(f'Value Function:\\n {Q.reshape(num_states, num_actions)}')\n",
    "print('\\n')\n",
    "print(f'Deterministic Policy:\\n {policy.reshape(rows, columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {0:'Up', 1:'Down', 2:'Right', 3:'Left'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
