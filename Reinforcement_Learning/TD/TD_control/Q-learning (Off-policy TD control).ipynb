{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "####################CLIFF WALKING ENVIRONMENT#########################\n",
    "\n",
    "A schematic view of the environment-\n",
    "\n",
    "o  o  o  o  o  o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o  o  o  o  o  o\n",
    "S  x  x  x  x  x  x  x  x  x  x  T\n",
    "\n",
    "Actions: \n",
    "    UP (0)\n",
    "    DOWN (1)\n",
    "    RIGHT (2)\n",
    "    LEFT (3)\n",
    "\n",
    "Rewards: \n",
    "     0 for going in Terminal state\n",
    "    -100 for falling in the cliff\n",
    "    -1 for all other actions in any state\n",
    "\n",
    "Note: State remains the same on going out of the maze (but -1 reward is given)\n",
    "      The episode ends and the agent returns to the start state after falling in the cliff\n",
    "\n",
    "'''\n",
    "START_STATE = 36\n",
    "TERMINAL_STATE = 47\n",
    "def reward(state):\n",
    "    if(state == TERMINAL_STATE):\n",
    "        reward = 0\n",
    "    elif(state > START_STATE and state < TERMINAL_STATE):\n",
    "        reward = -100\n",
    "    else:\n",
    "        reward = -1\n",
    "    return reward\n",
    "\n",
    "def env(state, action):\n",
    "    # return_val = [prob, next state, reward, isdone]\n",
    "    num_states = rows * columns\n",
    "    isdone = lambda state: state > START_STATE and state <= TERMINAL_STATE\n",
    "    \n",
    "    if(isdone(state)):\n",
    "        next_state = state\n",
    "    else:\n",
    "        if(action==0):\n",
    "            next_state = state-columns if state-columns>=0 else state\n",
    "        elif(action==1):\n",
    "            next_state = state+columns if state+columns<num_states else state\n",
    "        elif(action==2):\n",
    "            next_state = state+1 if (state+1)%columns else state\n",
    "        elif(action==3):\n",
    "            next_state = state-1 if state%columns else state \n",
    "    # State Transition Probability is 1 because the environment is deterministic\n",
    "    return_val = [1, next_state, reward(next_state), isdone(next_state)]\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Learning Rate\n",
    "epsilon = 0.1 # For Epsilon-greedy policy to balance exploration and exploitation\n",
    "rows = 4\n",
    "columns = 12\n",
    "num_states = rows * columns\n",
    "num_actions = 4\n",
    "gamma = 1 # Discount Factor\n",
    "episodes = 100000 # Number of games played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning():\n",
    "    # Initialize the action value function\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    for episode in range(episodes):\n",
    "        # Initialize S\n",
    "        curr_state = START_STATE\n",
    "        while True:\n",
    "            # Generate a random number between 0 and 1\n",
    "            P = np.random.random()\n",
    "            if(P > epsilon):\n",
    "                # Pick the greedy action\n",
    "                curr_action = np.argmax(Q[curr_state])\n",
    "            else:\n",
    "                # Pick a random action to explore\n",
    "                curr_action = np.random.randint(0, num_actions)\n",
    "            # prob: State Transition Probability \n",
    "            # reward, next_state: Immediate reward and next state on taking curr_action in curr_state\n",
    "            # isdone: Whether the next state is Terminal or not    \n",
    "            prob, next_state, reward, isdone = env(curr_state, curr_action)\n",
    "            # Update the current state-action value\n",
    "            Q[curr_state, curr_action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[curr_state, curr_action])\n",
    "            curr_state = next_state\n",
    "            if isdone:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      " [[ -12.96438022  -12.89352392  -12.89723806  -12.95246411]\n",
      " [ -12.06702773  -11.97597529  -11.97464032  -12.3063454 ]\n",
      " [ -11.28379205  -10.99074828  -10.99051255  -11.60347591]\n",
      " [ -10.13001207   -9.99630553   -9.99589533  -10.96812538]\n",
      " [  -9.22874498   -8.99824401   -8.99829562   -9.74899718]\n",
      " [  -8.5260799    -7.99919384   -7.9991563    -8.58422317]\n",
      " [  -7.37981913   -6.99960567   -6.99958835   -7.78854913]\n",
      " [  -6.36248133   -5.9998284    -5.99981456   -7.06538996]\n",
      " [  -5.36731915   -4.99994338   -4.99994586   -6.35806051]\n",
      " [  -4.38089186   -3.9999833    -3.99998309   -5.23763811]\n",
      " [  -3.51467045   -2.99999647   -2.99999678   -3.650619  ]\n",
      " [  -2.26462869   -2.           -2.09981      -2.50448295]\n",
      " [ -13.81426218  -12.          -12.          -12.99968731]\n",
      " [ -12.93203483  -11.          -11.          -12.99928642]\n",
      " [ -11.96700495  -10.          -10.          -11.9968391 ]\n",
      " [ -10.98624184   -9.           -9.          -10.99954981]\n",
      " [  -9.98854161   -8.           -8.           -9.99754358]\n",
      " [  -8.99599233   -7.           -7.           -8.99978144]\n",
      " [  -7.99288127   -6.           -6.           -7.9995028 ]\n",
      " [  -6.99698891   -5.           -5.           -6.99957549]\n",
      " [  -5.99919417   -4.           -4.           -5.99943142]\n",
      " [  -4.99926329   -3.           -3.           -4.99883877]\n",
      " [  -3.99822931   -2.           -2.           -3.99308188]\n",
      " [  -2.99312403   -1.           -1.99980219   -2.9974396 ]\n",
      " [ -13.          -13.          -11.          -12.        ]\n",
      " [ -12.         -100.          -10.          -12.        ]\n",
      " [ -11.         -100.           -9.          -11.        ]\n",
      " [ -10.         -100.           -8.          -10.        ]\n",
      " [  -9.         -100.           -7.           -9.        ]\n",
      " [  -8.         -100.           -6.           -8.        ]\n",
      " [  -7.         -100.           -5.           -7.        ]\n",
      " [  -6.         -100.           -4.           -6.        ]\n",
      " [  -5.         -100.           -3.           -5.        ]\n",
      " [  -4.         -100.           -2.           -4.        ]\n",
      " [  -3.         -100.           -1.           -3.        ]\n",
      " [  -2.            0.           -1.           -2.        ]\n",
      " [ -12.          -13.         -100.          -13.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "\n",
      "\n",
      "Deterministic Policy:\n",
      " [[1 2 2 2 1 2 2 2 1 2 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Q = qlearning()\n",
    "# Deterministic policy obtained using updated Q values\n",
    "policy = np.argmax(Q,axis=1)\n",
    "print(f'Value Function:\\n {Q.reshape(num_states, num_actions)}')\n",
    "print('\\n')\n",
    "print(f'Deterministic Policy:\\n {policy.reshape(rows, columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {0:'Up', 1:'Down', 2:'Right', 3:'Left'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
