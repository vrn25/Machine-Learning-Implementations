{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "####################CLIFF WALKING ENVIRONMENT#########################\n",
    "\n",
    "A schematic view of the environment-\n",
    "\n",
    "o  o  o  o  o  o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o  o  o  o  o  o\n",
    "S  x  x  x  x  x  x  x  x  x  x  T\n",
    "\n",
    "Actions: \n",
    "    UP (0)\n",
    "    DOWN (1)\n",
    "    RIGHT (2)\n",
    "    LEFT (3)\n",
    "\n",
    "Rewards: \n",
    "     0 for going in Terminal state\n",
    "    -100 for falling in the cliff\n",
    "    -1 for all other actions in any state\n",
    "\n",
    "Note: State remains the same on going out of the maze (but -1 reward is given)\n",
    "      The episode ends and the agent returns to the start state after falling in the cliff\n",
    "\n",
    "'''\n",
    "START_STATE = 36\n",
    "TERMINAL_STATE = 47\n",
    "def reward(state):\n",
    "    if(state == TERMINAL_STATE):\n",
    "        reward = 0\n",
    "    elif(state > START_STATE and state < TERMINAL_STATE):\n",
    "        reward = -100\n",
    "    else:\n",
    "        reward = -1\n",
    "    return reward\n",
    "\n",
    "def env(state, action):\n",
    "    # return_val = [prob, next state, reward, isdone]\n",
    "    num_states = rows * columns\n",
    "    isdone = lambda state: state > START_STATE and state <= TERMINAL_STATE\n",
    "    \n",
    "    if(isdone(state)):\n",
    "        next_state = state\n",
    "    else:\n",
    "        if(action==0):\n",
    "            next_state = state-columns if state-columns>=0 else state\n",
    "        elif(action==1):\n",
    "            next_state = state+columns if state+columns<num_states else state\n",
    "        elif(action==2):\n",
    "            next_state = state+1 if (state+1)%columns else state\n",
    "        elif(action==3):\n",
    "            next_state = state-1 if state%columns else state \n",
    "    # State Transition Probability is 1 because the environment is deterministic\n",
    "    return_val = [1, next_state, reward(next_state), isdone(next_state)]\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Learning Rate\n",
    "epsilon = 0.1 # For Epsilon-greedy policy to balance exploration and exploitation\n",
    "rows = 4\n",
    "columns = 12\n",
    "num_states = rows * columns\n",
    "num_actions = 4\n",
    "gamma = 1 # Discount Factor\n",
    "episodes = 100000 # Number of games played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa():\n",
    "    # Initialize the action value function\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    for episode in range(episodes):\n",
    "        # Initialize S\n",
    "        curr_state = START_STATE\n",
    "        # Pick a random number between 0 and 1\n",
    "        P = np.random.random()\n",
    "        if(P > epsilon):\n",
    "            # Pick the greedy action\n",
    "            curr_action = np.argmax(Q[curr_state])\n",
    "        else:\n",
    "            # Pick a random action to explore\n",
    "            curr_action = np.random.randint(0, num_actions)\n",
    "        while True:\n",
    "            # prob: State Transition Probability \n",
    "            # reward, next_state: Immediate reward and next state on taking curr_action in curr_state\n",
    "            # isdone: Whether the next state is Terminal or not\n",
    "            prob, next_state, reward, isdone = env(curr_state, curr_action)\n",
    "            # Pick a random number between 0 and 1\n",
    "            P = np.random.random()\n",
    "            if(P > epsilon):\n",
    "                # Pick the greedy action\n",
    "                next_action = np.argmax(Q[next_state])\n",
    "            else:\n",
    "                # Pick a random action to explore\n",
    "                next_action = np.random.randint(0, num_actions)\n",
    "            # Update the current state-action value\n",
    "            Q[curr_state, curr_action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[curr_state, curr_action])\n",
    "            curr_state = next_state\n",
    "            curr_action = next_action\n",
    "            if isdone:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function for SARSA:\n",
      " [[ -15.94559988  -16.9948888   -14.69603306  -15.72345508]\n",
      " [ -15.05465572  -15.76705586  -13.54840985  -15.87710741]\n",
      " [ -13.45242819  -14.69868729  -12.16853176  -14.94160782]\n",
      " [ -12.63413295  -13.699749    -10.96799573  -13.63458863]\n",
      " [ -11.61395304  -13.18976409  -10.12519889  -12.66692852]\n",
      " [ -10.21356605  -11.41455096   -8.81595204  -11.54811983]\n",
      " [  -8.89894462   -9.44245617   -7.84409062  -10.38606989]\n",
      " [  -7.97057786   -7.50709271   -6.86752574   -9.04047756]\n",
      " [  -6.71601565   -6.28608529   -5.76857999   -8.10824048]\n",
      " [  -5.75416482   -6.05996278   -4.48956925   -6.89134512]\n",
      " [  -4.53879864   -3.37399566   -3.69865185   -5.72865438]\n",
      " [  -3.29939681   -2.29040333   -3.38761926   -4.6112411 ]\n",
      " [ -15.77520247  -18.15055322  -16.08927591  -16.81982097]\n",
      " [ -14.82251886  -20.91290962  -15.2901838   -17.36156562]\n",
      " [ -13.41420048  -19.27629725  -13.80332536  -15.83643976]\n",
      " [ -12.2886318   -14.46490268  -13.37879625  -14.82369556]\n",
      " [ -11.34638929  -18.07730175  -12.99867283  -13.01454097]\n",
      " [ -10.05664622  -20.45599566  -10.73889774  -11.73584902]\n",
      " [  -9.03976897  -16.21977817   -7.58169294  -10.58614222]\n",
      " [  -7.93519245   -8.3894575    -6.01766      -8.55182664]\n",
      " [  -6.85737218   -7.28799262   -4.91850924   -7.70474178]\n",
      " [  -5.5690163   -10.53676369   -3.66083772   -6.42503408]\n",
      " [  -4.69863053   -3.32842077   -2.19363753   -4.66993645]\n",
      " [  -3.54830167   -1.03148761   -2.19113083   -3.50378001]\n",
      " [ -16.76027556  -23.30124192  -19.54379671  -18.37898227]\n",
      " [ -15.6939623   -99.99994615  -19.62986193  -17.61662049]\n",
      " [ -14.68592049  -98.66972054  -19.19575655  -15.8211625 ]\n",
      " [ -13.55443885  -98.17519964  -15.045836    -14.57543324]\n",
      " [ -12.14161168  -95.76088417  -18.81824128  -13.62541004]\n",
      " [ -12.42065446  -93.53891811  -12.56506205  -12.42885287]\n",
      " [  -8.64001917  -94.76652367  -10.40188297  -11.07638881]\n",
      " [  -7.54225118  -93.53891811  -11.28587451   -9.62520034]\n",
      " [  -6.03266808  -98.35767967  -11.51128787   -8.70045731]\n",
      " [  -4.6098461   -97.97244404   -8.87336206   -9.30967013]\n",
      " [  -3.42441529  -99.99989866   -1.01445727  -13.83482973]\n",
      " [  -2.58604241    0.           -1.11134146   -2.48612962]\n",
      " [ -17.78781573  -24.29407585 -100.          -19.33788316]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "\n",
      "\n",
      "Deterministic Policy:\n",
      " [[2 2 2 2 2 2 2 2 2 2 1 1]\n",
      " [0 0 0 0 0 0 2 2 2 2 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Q = sarsa()\n",
    "# Deterministic policy obtained using updated Q values\n",
    "policy = np.argmax(Q,axis=1)\n",
    "print(f'Value Function for SARSA:\\n {Q.reshape(num_states, num_actions)}')\n",
    "print('\\n')\n",
    "print(f'Deterministic Policy:\\n {policy.reshape(rows, columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {0:'Up', 1:'Down', 2:'Right', 3:'Left'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
