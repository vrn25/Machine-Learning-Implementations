{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "####################CLIFF WALKING ENVIRONMENT#########################\n",
    "\n",
    "A schematic view of the environment-\n",
    "\n",
    "o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o\n",
    "S  x  x  x  x  x  T\n",
    "\n",
    "Actions: \n",
    "    UP (0)\n",
    "    DOWN (1)\n",
    "    RIGHT (2)\n",
    "    LEFT (3)\n",
    "\n",
    "Rewards: \n",
    "     0 for going in Terminal state\n",
    "    -100 for falling in the cliff\n",
    "    -1 for all other actions in any state\n",
    "\n",
    "Note: State remains the same on going out of the maze (but -1 reward is given)\n",
    "      The episode ends and the agent returns to the start state after falling in the cliff\n",
    "\n",
    "'''\n",
    "START_STATE = 36\n",
    "TERMINAL_STATE = 47\n",
    "def reward(state):\n",
    "    if(state == TERMINAL_STATE):\n",
    "        reward = 0\n",
    "    elif(state > START_STATE and state < TERMINAL_STATE):\n",
    "        reward = -100\n",
    "    else:\n",
    "        reward = -1\n",
    "    return reward\n",
    "\n",
    "def env(state, action):\n",
    "    # return_val = [prob, next state, reward, isdone]\n",
    "    num_states = rows * columns\n",
    "    isdone = lambda state: state > START_STATE and state <= TERMINAL_STATE\n",
    "    \n",
    "    if(isdone(state)):\n",
    "        next_state = state\n",
    "    else:\n",
    "        if(action==0):\n",
    "            next_state = state-columns if state-columns>=0 else state\n",
    "        elif(action==1):\n",
    "            next_state = state+columns if state+columns<num_states else state\n",
    "        elif(action==2):\n",
    "            next_state = state+1 if (state+1)%columns else state\n",
    "        elif(action==3):\n",
    "            next_state = state-1 if state%columns else state \n",
    "    # State Transition Probability is 1 because the environment is deterministic\n",
    "    return_val = [1, next_state, reward(next_state), isdone(next_state)]\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Learning Rate\n",
    "epsilon = 0.1 # For Epsilon-greedy policy to balance exploration and exploitation\n",
    "rows = 4\n",
    "columns = 12\n",
    "num_states = rows * columns\n",
    "num_actions = 4\n",
    "gamma = 1 # Discount Factor\n",
    "episodes = 100000 # Number of games played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa():\n",
    "    # Initialize the action value function\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    for episode in range(episodes):\n",
    "        # Initialize S\n",
    "        curr_state = START_STATE\n",
    "        # Pick a random number between 0 and 1\n",
    "        P = np.random.random()\n",
    "        if(P > epsilon):\n",
    "            # Pick the greedy action\n",
    "            curr_action = np.argmax(Q[curr_state])\n",
    "        else:\n",
    "            # Pick a random action to explore\n",
    "            curr_action = np.random.randint(0, num_actions)\n",
    "        while True:\n",
    "            # prob: State Transition Probability \n",
    "            # reward, next_state: Immediate reward and next state on taking curr_action in curr_state\n",
    "            # isdone: Whether the next state is Terminal or not\n",
    "            prob, next_state, reward, isdone = env(curr_state, curr_action)\n",
    "            # Pick a random number between 0 and 1\n",
    "            P = np.random.random()\n",
    "            if(P > epsilon):\n",
    "                # Pick the greedy action\n",
    "                next_action = np.argmax(Q[next_state])\n",
    "            else:\n",
    "                # Pick a random action to explore\n",
    "                next_action = np.random.randint(0, num_actions)\n",
    "            # Update the current state-action value\n",
    "            Q[curr_state, curr_action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[curr_state, curr_action])\n",
    "            curr_state = next_state\n",
    "            curr_action = next_action\n",
    "            if isdone:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function for SARSA:\n",
      " [[ -15.60555687  -16.96641271  -14.53493899  -15.73252715]\n",
      " [ -14.49554434  -15.36761577  -13.50622921  -15.7150597 ]\n",
      " [ -13.23100905  -14.4058423   -12.16701437  -14.53295737]\n",
      " [ -12.14177393  -12.13892805  -11.23893725  -13.66379029]\n",
      " [ -11.01564949  -10.73712911  -10.01043216  -12.4128524 ]\n",
      " [  -9.99746244   -9.51264392   -8.75839051  -11.20221966]\n",
      " [  -8.76153989   -8.49614833   -7.59918664  -10.16049443]\n",
      " [  -7.81443012   -7.27031718   -6.56318363   -9.11688528]\n",
      " [  -6.57847282   -5.91881158   -5.4960741    -7.97106281]\n",
      " [  -5.78924386   -4.77733112   -4.5228535    -6.94935889]\n",
      " [  -4.47987028   -3.57070257   -3.34761424   -5.94305141]\n",
      " [  -3.51003363   -2.61424542   -3.3738531    -4.73072512]\n",
      " [ -15.50078571  -18.10493193  -16.06252619  -16.57097846]\n",
      " [ -14.61090049  -17.56109788  -14.29913518  -16.61421957]\n",
      " [ -13.37697914  -14.93443129  -13.39354137  -15.07501876]\n",
      " [ -12.28509083  -17.37021221  -10.85345724  -13.57757109]\n",
      " [ -11.10047832  -17.27792207   -9.8947325   -12.36742615]\n",
      " [  -9.94766733  -23.37682017   -8.07677635  -11.71624867]\n",
      " [  -9.05319386  -11.46548916   -7.21335904   -9.70068295]\n",
      " [  -7.76553641   -9.0895103    -5.53147663   -8.82049245]\n",
      " [  -6.72159319   -7.74056941   -4.36214452   -7.29206758]\n",
      " [  -5.61133221   -6.06288589   -3.20327232   -6.23779967]\n",
      " [  -4.54391317   -3.97353947   -2.22370463   -4.83402184]\n",
      " [  -3.55586566   -1.53490191   -2.19804002   -3.83415647]\n",
      " [ -16.55335402  -19.03920285  -22.84258405  -17.88837955]\n",
      " [ -15.49549405  -99.99999795  -16.49461548  -17.46515761]\n",
      " [ -14.04996846  -96.90968456  -16.41894681  -17.10388009]\n",
      " [ -12.86184622  -97.21871611  -19.90167789  -14.99893428]\n",
      " [ -10.82404562  -98.35767967  -13.87329139  -13.61271855]\n",
      " [  -9.49803294  -97.97244404  -19.02936395  -12.79582846]\n",
      " [  -8.27363646  -95.2898713   -10.16142084  -11.17131736]\n",
      " [  -7.2212316   -97.74716005  -12.88092478  -10.85032371]\n",
      " [  -6.02287426  -96.90968456  -10.61812824   -8.85584721]\n",
      " [  -4.58368261  -98.52191171   -8.4307422    -7.15783695]\n",
      " [  -3.41819942  -99.99939236   -1.29620756   -7.91161571]\n",
      " [  -2.42411738    0.           -1.21610831   -2.07701398]\n",
      " [ -17.76151577  -26.25753194 -100.          -20.83573506]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "\n",
      "\n",
      "Deterministic Policy:\n",
      " [[2 2 2 2 2 2 2 2 2 2 2 1]\n",
      " [0 2 0 2 2 2 2 2 2 2 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Q = sarsa()\n",
    "# Deterministic policy obtained using updated Q values\n",
    "policy = np.argmax(Q,axis=1)\n",
    "print(f'Value Function for SARSA:\\n {Q.reshape(num_states, num_actions)}')\n",
    "print('\\n')\n",
    "print(f'Deterministic Policy:\\n {policy.reshape(rows, columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {0:'Up', 1:'Down', 2:'Right', 3:'Left'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
